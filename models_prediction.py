# -*- coding: utf-8 -*-
"""models_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1quYTkp2x64ubhHJZ3NuuGa_846VPTe6M

**Описание данных**

- survival	| Выживание	0 = Нет, 1 = Да
 - pclass	| Класс билетов	1 = 1-й, 2 = 2-й, 3 = 3-й
 - sex	| Пол	
 - Age	| Возраст в годах	
 - sibsp	| Количество братьев и сестер / супругов на борту Титаника	
 - parch	| Количество родителей / детей на борту Титаника	
 - ticket	| Номер билета
 - fare	| Пассажирский тариф	
 - cabin	| Номер каюты	
 - embarked	| Порт посадки C = Cherbourg, Q = Queenstown, S = Southampton
"""

import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns
from collections import Counter

df = pd.read_csv('Titanik.csv')

df.info()

df.isnull().sum()

df['Age'].fillna(df['Age'].dropna().mean(), inplace = True)

df['Embarked'].fillna(df['Embarked'].mode()[0], inplace = True)

df['Cabin'].fillna(0, inplace = True)

df.isnull().sum()

df.describe()

plt.figure(figsize = (12, 6))
for col in df.columns:
  if df[col].dtype != 'object':
    df[col].plot(kind = 'hist', density = True, histtype = 'step', linewidth = 4)
    print(col)
    plt.show()

plt.figure(figsize = (12, 6))
for col in df.columns:
  if df[col].dtype != 'object':
    df[col].plot(kind = 'hist', density = True, histtype = 'step', linewidth = 4)
    print(col)
    plt.show()

df.Cabin

df['Cabin'] = df['Cabin'].replace(0, '0')

df['cabinNumber'] = df['Cabin'].apply(lambda x : x[1:] if x != '0' else '0')
df['numberForPerson'] = df['Cabin'].apply(lambda x : len(x.split(' ')) if x != '0' else 0)
df['place'] = df['Cabin'].apply(lambda x : x[0] if x != '0' else '0')
df['zero'] = df['Cabin'].apply(lambda x : 1 if x != '0' else 0)

df.tail(20)

Counter(df['numberForPerson'])

df.Name
df['status'] = df['Name'].apply(lambda x : x.split(' ')[1])
df['lenName'] = df['Name'].apply(lambda x : len(x))
df['countWordsInName'] = df['Name'].apply(lambda x : len(x.split(' ')))

df.head(20)

df['status']

Counter(df['status'])

plt.figure(figsize = (12, 6))
for col in df.columns:
  if df[col].dtype != 'object':
    df[col].plot(kind = 'hist', density = True, histtype = 'step', linewidth = 4)
    print(col)
    plt.show()

plt.figure(figsize = (12, 6))
for col in df.columns:
  if df[col].dtype == 'object':
    df[col].value_counts(normalize = True).plot(kind = 'bar')
    print(col)
    plt.show()

df.info()

df1=df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'numberForPerson', 'place', 'zero', 'status', 'lenName', 'countWordsInName']]
df2=pd.get_dummies(df1, drop_first = True)
df2.head()

df2.to_csv("Titanik_all.csv")

"""# Model"""

from sklearn.preprocessing import LabelEncoder

for col in df.columns: 
  if df[col].dtype == 'object':# and col != 'Cabin' and col != 'cabinNumber':
    print(col, 'transforming')
    encoder = LabelEncoder()
    df[col] = encoder.fit_transform(df[col]) 
    print(col, 'transformed')

df.head()

df.columns

X = df[['Pclass', 'Sex', 'Age', 'SibSp',
       'Parch', 'Fare', 'Embarked',
       'place', 'zero', 'countWordsInName']]
y = df['Survived']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)

from sklearn.tree import DecisionTreeClassifier
dtc = DecisionTreeClassifier(random_state=42)

dtc.fit(X_train, y_train)

test_pred = dtc.predict(X_test)

X.columns

dtc.feature_importances_

from sklearn.ensemble import RandomForestClassifier as rf
rfc = rf(n_estimators = 300, random_state = 42)

rfc.fit(X_train, y_train)

test_pred1 = rfc.predict(X_test)

rfc.feature_importances_

from xgboost import XGBClassifier
xgbc = XGBClassifier(n_estimators = 50)

xgbc.fit(X_train, y_train)

test_pred2 = xgbc.predict(X_test)

xgbc.feature_importances_

from sklearn.metrics import recall_score, precision_score, f1_score, roc_auc_score, accuracy_score, confusion_matrix 
metrics = [recall_score, precision_score, f1_score, roc_auc_score, accuracy_score, confusion_matrix]

for metric in metrics:
  print(metric, metric(y_test, test_pred).round(2))

for metric in metrics:
  print(metric, metric(y_test, test_pred1).round(2))

for metric in metrics:
  print(metric, metric(y_test, test_pred2).round(2))

for metric in metrics:
  print(metric, 'dtc', metric(y_test, test_pred).round(2), 'rf', metric(y_test, test_pred1).round(2), 'xgb', metric(y_test, test_pred2).round(2))

df.to_csv("Titanik_encodered.csv")

"""# **Нейросеть**"""

from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

def creat_nn(dataframe):
  '''
  dataframe: выборка
  output: создаем нейронную сеть
  '''
  # создание пустой нейронной сети
  nn = Sequential()
  # relu ~ rectified linear unit
  # слой входных параметров
  nn.add(Dense(dataframe.shape[1], activation='relu'))
  # второй скрытый слов
  nn.add(Dense(dataframe.shape[1], activation='relu')) 
  # выходной слов
  nn.add(Dense(1, activation='sigmoid'))
  # 
  nn.compile(loss='binary_crossentropy', optimizer='adam')
  return nn

df.head()

# выделим матрицу признаков
X1 = df.drop(labels=['Survived'], axis=1).values
# вектор целевого признака
y1 = df['Survived'].values

# 25 процентов на тестовое подмножество
# 75 процентов на тренировочное подмножество
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.25, random_state=1)

# обучаем нейронную сеть на тренировочном подмножестве
nn_1 = creat_nn(dataframe=df)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# nn_1.fit(X1_train, y1_train, epochs=100, validation_data=(X1_test, y1_test))

# задаем границу циклов обучения нейронной сети
from tensorflow.keras.callbacks import EarlyStopping

# наблюдаем за минизацией функции потери на валидационном множестве,
# и если после 20 циклов (эпох) не снижается величина функции потери,
# то останавливаем обучение нейронной сети
early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=20)

nn_2 = creat_nn(dataframe=df)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# nn_2.fit(X1_train, y1_train, validation_data=(X1_test, y1_test), epochs=300,
#          callbacks=[early_stop], verbose=0)

# сохраняем величины функции потерь тренировочного и тестового множеств
losses_3 = pd.DataFrame(nn_2.history.history)

# графическая иллюстрация динамики изменения функции потерь
losses_3.plot(figsize=(12,6))

# Commented out IPython magic to ensure Python compatibility.
# загружаем библиотеки
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

predict_x=nn_2.predict(X1_test) 
classes_x=np.argmax(predict_x,axis=1)

from sklearn.metrics import accuracy_score

# определяем точность нейронной сети
accuracy_score(y1_test, classes_x)

"""# **Логистическая регрессия**"""

from sklearn.linear_model import LogisticRegression

X2 = df.drop(labels = ["Survived"], axis = 1)

Y2 = df["Survived"]

X2_train, X2_test, y2_train, y2_test = train_test_split(X2, Y2, test_size = 0.25, random_state = 2)

log_reg = LogisticRegression().fit(X2_train,y2_train)
log_reg.score(X2_train,y2_train)

log_reg.score(X2_test,y2_test)